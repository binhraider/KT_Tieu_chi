import pandas as pd
import mysql.connector
from mysql.connector import Error
import re
import logging
from datetime import datetime

# C·∫•u h√¨nh logging chi ti·∫øt
logging.basicConfig(
    filename=f'import_log_{datetime.now().strftime("%Y%m%d_%H%M%S")}.txt', 
    level=logging.INFO, 
    format='%(asctime)s - %(levelname)s - %(message)s', 
    filemode='w'
)

# Console handler ƒë·ªÉ hi·ªÉn th·ªã log tr√™n m√†n h√¨nh
console_handler = logging.StreamHandler()
console_handler.setLevel(logging.WARNING)
formatter = logging.Formatter('%(levelname)s - %(message)s')
console_handler.setFormatter(formatter)
logging.getLogger().addHandler(console_handler)

# --- THAY ƒê·ªîI C√ÅC TH√îNG S·ªê K·∫æT N·ªêI DATABASE C·ª¶A B·∫†N T·∫†I ƒê√ÇY ---
DB_CONFIG = {
    'host': 'localhost',
    'database': 'hoso_khoahoc_db',
    'user': 'root',
    'password': '1111',
    'charset': 'utf8mb4',
    'collation': 'utf8mb4_unicode_ci'
}

def create_db_connection():
    """T·∫°o k·∫øt n·ªëi ƒë·∫øn database v·ªõi error handling t·ªët h∆°n."""
    connection = None
    try:
        connection = mysql.connector.connect(**DB_CONFIG)
        print("‚úÖ K·∫øt n·ªëi ƒë·∫øn MySQL th√†nh c√¥ng!")
        logging.info("K·∫øt n·ªëi database th√†nh c√¥ng")
    except Error as e:
        error_msg = f"L·ªói khi k·∫øt n·ªëi ƒë·∫øn MySQL: {e}"
        print(f"‚ùå {error_msg}")
        logging.error(error_msg)
    return connection

def create_tables(connection):
    """T·∫°o b·∫£ng journals v√† journal_points v·ªõi c·∫•u tr√∫c t·ªëi ∆∞u."""
    cursor = connection.cursor()
    
    try:
        # B·∫£ng journals v·ªõi ƒë·∫ßy ƒë·ªß c·ªôt v√† t·ªëi ∆∞u h√≥a
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS journals (
                id INT UNSIGNED AUTO_INCREMENT PRIMARY KEY,
                name TEXT NOT NULL,
                issn VARCHAR(255) NULL,
                eissn VARCHAR(255) NULL,
                publisher TEXT NULL,
                type VARCHAR(255) NULL,
                link VARCHAR(2048) NULL,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
                INDEX idx_issn (issn),
                INDEX idx_eissn (eissn)
            ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci;
        """)
        
        # B·∫£ng journal_points v·ªõi c·∫•u tr√∫c t·ªëi ∆∞u
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS journal_points (
                id INT UNSIGNED AUTO_INCREMENT PRIMARY KEY,
                journal_id INT UNSIGNED NOT NULL,
                field_of_study VARCHAR(255) NULL,
                points DECIMAL(5, 2) NOT NULL,
                publication_type VARCHAR(50) NULL,
                effective_from DATE NOT NULL,
                effective_to DATE NULL,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
                FOREIGN KEY (journal_id) REFERENCES journals(id) ON DELETE CASCADE,
                INDEX idx_journal_id (journal_id),
                INDEX idx_dates (effective_from, effective_to),
                INDEX idx_field (field_of_study)
            ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci;
        """)
        
        connection.commit()
        print("‚úÖ ƒê√£ ki·ªÉm tra/t·∫°o b·∫£ng journals v√† journal_points v·ªõi indexes t·ªëi ∆∞u.")
        logging.info("T·∫°o b·∫£ng th√†nh c√¥ng")
        
    except Error as e:
        error_msg = f"L·ªói khi t·∫°o b·∫£ng: {e}"
        print(f"‚ùå {error_msg}")
        logging.error(error_msg)
        raise
    finally:
        cursor.close()

def normalize_issn(issn_string):
    """
    Chu·∫©n h√≥a ISSN/eISSN v·ªõi logic k·∫øt h·ª£p t·ª´ c·∫£ 2 phi√™n b·∫£n.
    X·ª≠ l√Ω c√°c tr∆∞·ªùng h·ª£p ph·ª©c t·∫°p v√† ƒë∆°n gi·∫£n.
    """
    if not issn_string or pd.isna(issn_string) or str(issn_string).lower() in ['none', 'n/a', '']:
        return None, None
    
    issn, eissn = None, None
    issn_string = str(issn_string).strip()
    
    # X·ª≠ l√Ω tr∆∞·ªùng h·ª£p c√≥ p- v√† e- (t·ª´ phi√™n b·∫£n 1)
    p_match = re.search(r'p-([\d\w-]+)', issn_string, re.IGNORECASE)
    e_match = re.search(r'e-([\d\w-]+)', issn_string, re.IGNORECASE)
    
    if p_match:
        issn = p_match.group(1).strip()
    if e_match:
        eissn = e_match.group(1).strip()
    
    # N·∫øu kh√¥ng t√¨m th·∫•y p- ho·∫∑c e-, s·ª≠ d·ª•ng logic t·ª´ phi√™n b·∫£n 2
    if not issn and not eissn:
        if ',' in issn_string:
            parts = issn_string.split(',')
            for part in parts:
                part = part.strip()
                if part.startswith('p-'):
                    issn = part.replace('p-', '').strip()
                elif part.startswith('e-'):
                    eissn = part.replace('e-', '').strip()
                elif not issn:  # G√°n part ƒë·∫ßu ti√™n l√†m issn n·∫øu ch∆∞a c√≥
                    issn = re.sub(r'[^0-9-]', '', part)
        else:
            # Tr∆∞·ªùng h·ª£p ƒë∆°n gi·∫£n - l·∫•y chu·ªói s·ªë-g·∫°ch ngang ƒë·∫ßu ti√™n
            main_match = re.search(r'([\d-]+)', issn_string)
            if main_match:
                issn = main_match.group(1).strip()
    
    # Validate ISSN format (basic check)
    def validate_issn(issn_val):
        if issn_val and re.match(r'^\d{4}-\d{3}[\dxX]$', issn_val):
            return issn_val
        elif issn_val and re.match(r'^\d{4}-\d{4}$', issn_val):
            return issn_val
        return issn_val  # Tr·∫£ v·ªÅ nh∆∞ c≈© n·∫øu kh√¥ng match format chu·∫©n
    
    return validate_issn(issn), validate_issn(eissn)

def parse_point_rules(point_string):
    """
    Ph√¢n t√≠ch chu·ªói ƒëi·ªÉm quy ƒë·ªïi v·ªõi logic k·∫øt h·ª£p v√† c·∫£i ti·∫øn.
    X·ª≠ l√Ω c√°c tr∆∞·ªùng h·ª£p ph·ª©c t·∫°p v√† edge cases.
    """
    if not point_string or pd.isna(point_string) or str(point_string).strip() == '':
        logging.warning(f"Chu·ªói ƒëi·ªÉm r·ªóng ho·∫∑c kh√¥ng h·ª£p l·ªá: {point_string}")
        return [{'points': 0.0, 'from': '1900-01-01', 'to': None, 'publication_type': None}]

    point_string = str(point_string).strip()
    
    # T√°ch c√°c quy t·∫Øc b·∫±ng d·∫•u ch·∫•m ph·∫©y
    rule_parts = [part.strip() for part in point_string.split(';') if part.strip()]
    if not rule_parts:
        logging.warning(f"Kh√¥ng t√¨m th·∫•y quy t·∫Øc ƒëi·ªÉm trong: {point_string}")
        return [{'points': 0.0, 'from': '1900-01-01', 'to': None, 'publication_type': None}]

    parsed_rules = []
    
    for part in rule_parts:
        # T√¨m nƒÉm b·∫Øt ƒë·∫ßu
        year_match = re.search(r't·ª´ (\d{4})', part)
        from_year = int(year_match.group(1)) if year_match else None

        # Lo·∫°i b·ªè ph·∫ßn 't·ª´ <nƒÉm>' ƒë·ªÉ x·ª≠ l√Ω ph·∫ßn c√≤n l·∫°i
        search_str = re.sub(r't·ª´ \d{4}', '', part).strip()

        # T√¨m publication_type
        publication_type = None
        if 'online' in search_str.lower():
            if 'kh√¥ng online' in search_str.lower():
                publication_type = 'Kh√¥ng online'
                search_str = re.sub(r'kh√¥ng online', '', search_str, flags=re.IGNORECASE).strip()
            else:
                publication_type = 'Online'
                search_str = re.sub(r'online', '', search_str, flags=re.IGNORECASE).strip()

        # T√¨m gi√° tr·ªã ƒëi·ªÉm (h·ªó tr·ª£ c·∫£ d·∫•u ph·∫©y v√† d·∫•u ch·∫•m)
        point_matches = re.findall(r'(\d+[,.]\d+|\d+)', search_str)
        
        if point_matches:
            # L·∫•y s·ªë cu·ªëi c√πng (th∆∞·ªùng l√† ƒëi·ªÉm)
            point_str = point_matches[-1].replace(',', '.')
            try:
                points = float(point_str)
                # Ki·ªÉm tra gi√° tr·ªã h·ª£p l·ªá  
                if points > 100:
                    logging.warning(f"Gi√° tr·ªã ƒëi·ªÉm c√≥ th·ªÉ kh√¥ng h·ª£p l·ªá (>100): {points} trong '{part}'")
                    # C√≥ th·ªÉ l√† l·ªói nh·∫≠p li·ªáu, th·ª≠ chia cho 1000 ho·∫∑c ƒë·∫∑t v·ªÅ 0
                    if points > 1000:
                        points = 0.0
                        logging.warning(f"ƒê·∫∑t ƒëi·ªÉm v·ªÅ 0 do gi√° tr·ªã qu√° l·ªõn: {point_str}")
            except ValueError:
                logging.warning(f"Kh√¥ng th·ªÉ chuy·ªÉn ƒë·ªïi ƒëi·ªÉm: {point_str} trong '{part}'")
                points = 0.0
        else:
            logging.warning(f"Kh√¥ng t√¨m th·∫•y gi√° tr·ªã ƒëi·ªÉm trong: '{part}'")
            points = 0.0

        parsed_rules.append({
            'points': points,
            'year': from_year,
            'publication_type': publication_type
        })

    # S·∫Øp x·∫øp theo nƒÉm v√† t√≠nh to√°n effective dates
    parsed_rules.sort(key=lambda x: x['year'] if x['year'] is not None else 0)

    final_rules = []
    for i, current_rule in enumerate(parsed_rules):
        from_date = f"{current_rule['year']}-01-01" if current_rule['year'] else '1900-01-01'
        to_date = None
        
        # T√¨m rule ti·∫øp theo c√≥ nƒÉm ƒë·ªÉ t√≠nh effective_to
        for j in range(i + 1, len(parsed_rules)):
            if parsed_rules[j]['year'] is not None:
                to_date = f"{parsed_rules[j]['year'] - 1}-12-31"
                break

        final_rules.append({
            'points': current_rule['points'],
            'from': from_date,
            'to': to_date,
            'publication_type': current_rule['publication_type']
        })
        
    return final_rules if final_rules else [{'points': 0.0, 'from': '1900-01-01', 'to': None, 'publication_type': None}]

def import_data_from_excel(connection, excel_path, sheet_name, clean_data=False):
    """
    Nh·∫≠p d·ªØ li·ªáu t·ª´ Excel v·ªõi batch processing v√† error handling t·ªëi ∆∞u.
    
    Args:
        connection: MySQL connection object
        excel_path: ƒê∆∞·ªùng d·∫´n file Excel
        sheet_name: T√™n sheet
        clean_data: C√≥ x√≥a d·ªØ li·ªáu c≈© kh√¥ng (default: False)
    """
    try:
        print(f"üìñ ƒêang ƒë·ªçc file Excel: {excel_path}")
        df = pd.read_excel(excel_path, sheet_name=sheet_name)
        df = df.where(pd.notna(df), None)
        print(f"‚úÖ ƒê·ªçc th√†nh c√¥ng {len(df)} d√≤ng d·ªØ li·ªáu")
        logging.info(f"ƒê·ªçc file Excel th√†nh c√¥ng: {len(df)} d√≤ng")
        
    except FileNotFoundError:
        error_msg = f"Kh√¥ng t√¨m th·∫•y file '{excel_path}'"
        print(f"‚ùå {error_msg}")
        logging.error(error_msg)
        return
    except Exception as e:
        error_msg = f"L·ªói khi ƒë·ªçc file Excel: {e}"
        print(f"‚ùå {error_msg}")
        logging.error(error_msg)
        return
    
    cursor = connection.cursor(buffered=True)
    
    # X√≥a d·ªØ li·ªáu c≈© n·∫øu ƒë∆∞·ª£c y√™u c·∫ßu
    if clean_data:
        print("üßπ ƒêang x√≥a d·ªØ li·ªáu c≈©...")
        cursor.execute("DELETE FROM journal_points")
        cursor.execute("DELETE FROM journals") 
        connection.commit()
        print("‚úÖ ƒê√£ x√≥a d·ªØ li·ªáu c≈©")
        logging.info("ƒê√£ x√≥a d·ªØ li·ªáu c≈©")
    
    # B∆Ø·ªöC 1: Nh·∫≠p d·ªØ li·ªáu journals
    print("\nüìù B∆∞·ªõc 1: B·∫Øt ƒë·∫ßu nh·∫≠p d·ªØ li·ªáu v√†o b·∫£ng `journals`...")
    
    unique_journals_df = df.dropna(subset=['Ch·ªâ s·ªë ISSN/eISSN']).drop_duplicates(subset=['Ch·ªâ s·ªë ISSN/eISSN'])
    journal_values = []
    journal_errors = []
    
    for index, row in unique_journals_df.iterrows():
        try:
            issn, eissn = normalize_issn(row['Ch·ªâ s·ªë ISSN/eISSN'])
            if not issn and not eissn:
                journal_errors.append(f"H√†ng {index}: Kh√¥ng c√≥ ISSN/eISSN h·ª£p l·ªá - {row.get('T√™n t·∫°p ch√≠/h·ªôi ngh·ªã', 'N/A')}")
                continue
            
            # Ki·ªÉm tra t·ªìn t·∫°i (c·∫£i ti·∫øn query)
            cursor.execute("""
                SELECT id FROM journals 
                WHERE (issn = %s AND %s IS NOT NULL) 
                   OR (eissn = %s AND %s IS NOT NULL)
                LIMIT 1
            """, (issn, issn, eissn, eissn))
            
            if cursor.fetchone() is None:
                journal_values.append((
                    row.get('T√™n t·∫°p ch√≠/h·ªôi ngh·ªã'),
                    issn,
                    eissn, 
                    row.get('C∆° quan xu·∫•t b·∫£n'),
                    row.get('Lo·∫°i'),
                    row.get('Link t·∫°p ch√≠/h·ªôi ngh·ªã')
                ))
                
        except Exception as e:
            error_msg = f"H√†ng {index} (journals): {e}"
            journal_errors.append(error_msg)
            logging.error(error_msg)
    
    # Batch insert cho journals
    journal_success_count = 0
    if journal_values:
        try:
            cursor.executemany("""
                INSERT INTO journals (name, issn, eissn, publisher, type, link) 
                VALUES (%s, %s, %s, %s, %s, %s)
            """, journal_values)
            journal_success_count = len(journal_values)
            connection.commit()
            print(f"‚úÖ ƒê√£ nh·∫≠p {journal_success_count} journals th√†nh c√¥ng")
            logging.info(f"Nh·∫≠p {journal_success_count} journals th√†nh c√¥ng")
            
        except Exception as e:
            error_msg = f"L·ªói batch insert journals: {e}"
            print(f"‚ùå {error_msg}")
            logging.error(error_msg)
            connection.rollback()
    
    if journal_errors:
        print(f"‚ö†Ô∏è  C√≥ {len(journal_errors)} l·ªói khi x·ª≠ l√Ω journals")
        for error in journal_errors[:5]:  # Ch·ªâ hi·ªÉn th·ªã 5 l·ªói ƒë·∫ßu
            print(f"   ‚Ä¢ {error}")
        if len(journal_errors) > 5:
            print(f"   ‚Ä¢ ... v√† {len(journal_errors) - 5} l·ªói kh√°c (xem log file)")
    
    # B∆Ø·ªöC 2: Nh·∫≠p d·ªØ li·ªáu journal_points  
    print("\nüìä B∆∞·ªõc 2: B·∫Øt ƒë·∫ßu nh·∫≠p d·ªØ li·ªáu v√†o b·∫£ng `journal_points`...")
    
    point_values = []
    point_errors = []
    
    for index, row in df.iterrows():
        try:
            issn, eissn = normalize_issn(row['Ch·ªâ s·ªë ISSN/eISSN'])
            if not issn and not eissn:
                continue
            
            # T√¨m journal_id
            cursor.execute("""
                SELECT id FROM journals 
                WHERE (issn = %s AND %s IS NOT NULL) 
                   OR (eissn = %s AND %s IS NOT NULL)
                LIMIT 1
            """, (issn, issn, eissn, eissn))
            
            result = cursor.fetchone()
            if result:
                journal_id = result[0]
                point_rules = parse_point_rules(row['Quy ƒë·ªïi ƒëi·ªÉm'])
                
                for rule in point_rules:
                    if rule['points'] is None or rule['points'] < 0:
                        point_errors.append(f"H√†ng {index}: Gi√° tr·ªã ƒëi·ªÉm kh√¥ng h·ª£p l·ªá ({rule['points']})")
                        continue
                        
                    point_values.append((
                        journal_id,
                        row.get('Ng√†nh/Li√™n ng√†nh'),
                        rule['points'],
                        rule['publication_type'],
                        rule['from'],
                        rule['to']
                    ))
            else:
                point_errors.append(f"H√†ng {index}: Kh√¥ng t√¨m th·∫•y journal cho ISSN/eISSN {issn}/{eissn}")
                
        except Exception as e:
            error_msg = f"H√†ng {index} (journal_points): {e}"
            point_errors.append(error_msg)
            logging.error(error_msg)
    
    # Batch insert cho journal_points
    point_success_count = 0
    if point_values:
        try:
            cursor.executemany("""
                INSERT INTO journal_points (journal_id, field_of_study, points, publication_type, effective_from, effective_to) 
                VALUES (%s, %s, %s, %s, %s, %s)
            """, point_values)
            point_success_count = len(point_values)
            connection.commit()
            print(f"‚úÖ ƒê√£ nh·∫≠p {point_success_count} journal_points th√†nh c√¥ng")
            logging.info(f"Nh·∫≠p {point_success_count} journal_points th√†nh c√¥ng")
            
        except Exception as e:
            error_msg = f"L·ªói batch insert journal_points: {e}"
            print(f"‚ùå {error_msg}")
            logging.error(error_msg)
            connection.rollback()
    
    if point_errors:
        print(f"‚ö†Ô∏è  C√≥ {len(point_errors)} l·ªói khi x·ª≠ l√Ω journal_points")
        for error in point_errors[:5]:
            print(f"   ‚Ä¢ {error}")
        if len(point_errors) > 5:
            print(f"   ‚Ä¢ ... v√† {len(point_errors) - 5} l·ªói kh√°c (xem log file)")
    
    cursor.close()
    
    # Th·ªëng k√™ k·∫øt qu·∫£
    print(f"\nüìà T·ªïng k·∫øt:")
    print(f"   ‚Ä¢ Journals: {journal_success_count} th√†nh c√¥ng, {len(journal_errors)} l·ªói")
    print(f"   ‚Ä¢ Journal Points: {point_success_count} th√†nh c√¥ng, {len(point_errors)} l·ªói")
    
    return {
        'journals_success': journal_success_count,
        'journals_errors': len(journal_errors),
        'points_success': point_success_count, 
        'points_errors': len(point_errors)
    }

def verify_data(connection):
    """Ki·ªÉm tra v√† th·ªëng k√™ d·ªØ li·ªáu trong database."""
    cursor = connection.cursor()
    
    print("\nüîç Ki·ªÉm tra d·ªØ li·ªáu trong database:")
    
    # ƒê·∫øm journals
    cursor.execute("SELECT COUNT(*) FROM journals")
    journal_count = cursor.fetchone()[0]
    print(f"   ‚Ä¢ S·ªë journals: {journal_count}")
    
    # ƒê·∫øm journal_points
    cursor.execute("SELECT COUNT(*) FROM journal_points") 
    point_count = cursor.fetchone()[0]
    print(f"   ‚Ä¢ S·ªë journal_points: {point_count}")
    
    # Th·ªëng k√™ theo publication_type
    cursor.execute("""
        SELECT publication_type, COUNT(*) 
        FROM journal_points 
        GROUP BY publication_type
    """)
    pub_types = cursor.fetchall()
    print(f"   ‚Ä¢ Ph√¢n lo·∫°i publication_type:")
    for pub_type, count in pub_types:
        print(f"     - {pub_type or 'NULL'}: {count}")
    
    # Th·ªëng k√™ theo kho·∫£ng ƒëi·ªÉm
    cursor.execute("""
        SELECT 
            CASE 
                WHEN points = 0 THEN '0 ƒëi·ªÉm'
                WHEN points > 0 AND points <= 1 THEN '0-1 ƒëi·ªÉm'
                WHEN points > 1 AND points <= 5 THEN '1-5 ƒëi·ªÉm'
                ELSE '>5 ƒëi·ªÉm'
            END as point_range,
            COUNT(*)
        FROM journal_points 
        GROUP BY point_range
        ORDER BY MIN(points)
    """)
    point_ranges = cursor.fetchall()
    print(f"   ‚Ä¢ Ph√¢n b·ªë ƒëi·ªÉm:")
    for range_name, count in point_ranges:
        print(f"     - {range_name}: {count}")
    
    cursor.close()

def backup_data(connection, backup_file=None):
    """T·∫°o backup d·ªØ li·ªáu ra file CSV."""
    if not backup_file:
        backup_file = f"journal_backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
    
    try:
        cursor = connection.cursor()
        cursor.execute("""
            SELECT 
                j.name, j.issn, j.eissn, j.publisher, j.type, j.link,
                jp.field_of_study, jp.points, jp.publication_type,
                jp.effective_from, jp.effective_to
            FROM journals j
            LEFT JOIN journal_points jp ON j.id = jp.journal_id
            ORDER BY j.name, jp.effective_from
        """)
        
        columns = ['name', 'issn', 'eissn', 'publisher', 'type', 'link', 
                  'field_of_study', 'points', 'publication_type', 
                  'effective_from', 'effective_to']
        
        df = pd.DataFrame(cursor.fetchall(), columns=columns)
        df.to_csv(backup_file, index=False, encoding='utf-8-sig')
        
        print(f"‚úÖ ƒê√£ t·∫°o backup: {backup_file}")
        logging.info(f"T·∫°o backup th√†nh c√¥ng: {backup_file}")
        cursor.close()
        
    except Exception as e:
        error_msg = f"L·ªói khi t·∫°o backup: {e}"
        print(f"‚ùå {error_msg}")
        logging.error(error_msg)

if __name__ == '__main__':
    print("üöÄ B·∫Øt ƒë·∫ßu qu√° tr√¨nh nh·∫≠p d·ªØ li·ªáu t·∫°p ch√≠ khoa h·ªçc")
    print("=" * 60)
    
    # K·∫øt n·ªëi database
    conn = create_db_connection()
    if not conn or not conn.is_connected():
        print("‚ùå Kh√¥ng th·ªÉ k·∫øt n·ªëi database. Tho√°t ch∆∞∆°ng tr√¨nh.")
        exit(1)
    
    try:
        # T·∫°o b·∫£ng
        create_tables(conn)
        
        # C·∫•u h√¨nh file ƒë·∫ßu v√†o
        EXCEL_FILE_PATH = 'C:/Users/Admin/Downloads/hoso_khoahoc_backend/scripts/data.xlsx'
        JOURNAL_SHEET_NAME = 'DS t·∫°p ch√≠'
        
        # H·ªèi ng∆∞·ªùi d√πng c√≥ mu·ªën x√≥a d·ªØ li·ªáu c≈© kh√¥ng
        clean_choice = input("\nü§î B·∫°n c√≥ mu·ªën x√≥a d·ªØ li·ªáu c≈© tr∆∞·ªõc khi nh·∫≠p kh√¥ng? (y/N): ").lower()
        clean_data = clean_choice in ['y', 'yes']
        
        if clean_data:
            print("‚ö†Ô∏è  S·∫Ω x√≥a t·∫•t c·∫£ d·ªØ li·ªáu c≈©!")
        else:
            print("‚ÑπÔ∏è  S·∫Ω gi·ªØ l·∫°i d·ªØ li·ªáu c≈© v√† ch·ªâ th√™m m·ªõi.")
        
        # Nh·∫≠p d·ªØ li·ªáu
        result = import_data_from_excel(conn, EXCEL_FILE_PATH, JOURNAL_SHEET_NAME, clean_data)
        
        if result:
            # Ki·ªÉm tra d·ªØ li·ªáu
            verify_data(conn)
            
            # T·∫°o backup
            backup_choice = input("\nüíæ B·∫°n c√≥ mu·ªën t·∫°o backup d·ªØ li·ªáu kh√¥ng? (Y/n): ").lower()
            if backup_choice not in ['n', 'no']:
                backup_data(conn)
        
        print("\nüéâ Ho√†n t·∫•t qu√° tr√¨nh nh·∫≠p d·ªØ li·ªáu!")
        print("üìã Chi ti·∫øt xem trong file log ƒë∆∞·ª£c t·∫°o.")
        
    except KeyboardInterrupt:
        print("\n‚èπÔ∏è  Ng∆∞·ªùi d√πng h·ªßy b·ªè qu√° tr√¨nh.")
        logging.warning("Ng∆∞·ªùi d√πng h·ªßy b·ªè qu√° tr√¨nh")
        
    except Exception as e:
        error_msg = f"L·ªói kh√¥ng mong mu·ªën: {e}"
        print(f"\nüí• {error_msg}")
        logging.error(error_msg)
        
    finally:
        if conn and conn.is_connected():
            conn.close()
            print("üîå ƒê√£ ƒë√≥ng k·∫øt n·ªëi database.")
            logging.info("ƒê√≥ng k·∫øt n·ªëi database")